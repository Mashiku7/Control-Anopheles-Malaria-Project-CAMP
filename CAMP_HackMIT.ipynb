{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAMP HackMIT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cs2Bgz5JNh9H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time series forecasting  maleria prevelance in Muleba using ARIMA"
      ]
    },
    {
      "metadata": {
        "id": "puq7p8FthHSQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.data import Dataset\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = '{:.1f}'.format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K8VTQpKijXO7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dateparse = lambda dates: pd.datetime.strptime(dates, '%Y-%m-%d')\n",
        "data = pd.read_csv( 'https://raw.githubusercontent.com/Mashiku7/Control-Anopheles-Malaria-Project-CAMP/master/CDO6199837732377EnviromentData.csv',  index_col='YEAR',  parse_dates = ['YEAR'], date_parser = dateparse)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JAOxL0KmjfWW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Examine Data"
      ]
    },
    {
      "metadata": {
        "id": "fyXd42B8jeaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3I2P9Uf3rcR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqJTe2Turk9R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts = data[\"ANOPHELES\"] \n",
        "ts.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4g-Wj8vssW4l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts['637290']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dxYlZwDmuM-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Checking the stations\n",
        "\n",
        "we have Station ID, Day(Time stamp) and Density time series data then create the time-series\n",
        "\n",
        "Mean = constant over all intervals.\n",
        "Variance = constant over all intervals."
      ]
    },
    {
      "metadata": {
        "id": "TLusRiw_uo8d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.plot(ts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cAvJnz3Du81b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "def test_stationarity(timeseries):\n",
        "    \n",
        "    #Determing rolling statistics\n",
        "    rolmean = timeseries.rolling(window=12).mean()\n",
        "    rolstd = timeseries.rolling(window=12).std()\n",
        "\n",
        "    #Plot rolling statistics:\n",
        "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "    \n",
        "    #Perform Dickey-Fuller test:\n",
        "    print 'Results of Dickey-Fuller Test:'\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print dfoutput"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qeX0q7jvKyF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_stationarity(ts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbE3OH7UvxuF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ts_log = np.log(ts)\n",
        "plt.plot(ts_log)\n",
        "ts_smooth = ts_log.rolling(window = 12).mean()\n",
        "plt.plot(ts_smooth, color = 'red')\n",
        "plt.plot(ts_log)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksry0mAYwLGn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Forecasting\n",
        "Lets make model on the TS after differencing as it is a very popular technique. Also, its relatively easier to add noise and seasonality back into predicted residuals in this case. Having performed the trend and seasonality estimation techniques, there can be two situations:\n",
        "\n",
        "A strictly stationary series with no dependence among the values. This is the easy case wherein we can model the residuals as white noise. But this is very rare.\n",
        "A series with significant dependence among values. In this case we need to use some statistical models like ARIMA to forecast the data.\n",
        "Let me give you a brief introduction to ARIMA. I won’t go into the technical details but you should understand these concepts in detail if you wish to apply them more effectively. ARIMA stands for Auto-Regressive Integrated Moving Averages. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
        "\n",
        "Number of AR (Auto-Regressive) terms (p): AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
        "Number of MA (Moving Average) terms (q): MA terms are lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value.\n",
        "Number of Differences (d): These are the number of nonseasonal differences, i.e. in this case we took the first order difference. So either we can pass that variable and put d=0 or pass the original variable and put d=1. Both will generate same results.\n",
        "Selecting p, q, and d values\n",
        "\n",
        "Autocorrelation Function (ACF): It is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-5’…’t2-5’ (t1-5 and t2 being end points).\n",
        "Partial Autocorrelation Function (PACF): This measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4."
      ]
    },
    {
      "metadata": {
        "id": "Uml00K3zwTXj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#ACF and PACF plots:\n",
        "from statsmodels.tsa.stattools import acf, pacf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wx-QTE1wexz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "lag_acf = acf(ts_diff, nlags=20)\n",
        "lag_pacf = pacf(ts_diff, nlags=20, method='ols')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPAQc39uwiix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Plot ACF: \n",
        "plt.subplot(121) \n",
        "plt.plot(lag_acf)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
        "plt.title('Autocorrelation Function')\n",
        "\n",
        "#Plot PACF:\n",
        "plt.subplot(122)\n",
        "plt.plot(lag_pacf)\n",
        "plt.axhline(y=0,linestyle='--',color='gray')\n",
        "plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
        "plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
        "plt.title('Partial Autocorrelation Function')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBa-24MTwqAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima_model import ARIMA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gf_0ztn_xsKq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "p = 2, q = 2\n",
        "\n",
        "Now AR, MA & ARIMA models for the data"
      ]
    },
    {
      "metadata": {
        "id": "MI-egsIowuJf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "AR Model"
      ]
    },
    {
      "metadata": {
        "id": "tFcNIxMUw42C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ARIMA(ts_log, order=(2, 1, 0))  \n",
        "results_AR = model.fit(disp=-1)  \n",
        "plt.plot(ts_diff)\n",
        "plt.plot(results_AR.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "At0tk3L7w9fU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MA model\n",
        "\n",
        "model = ARIMA(ts_log, order=(0, 1, 2))  \n",
        "results_AR = model.fit(disp=-1)  \n",
        "plt.plot(ts_diff)\n",
        "plt.plot(results_AR.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rP_dt4t7yDIU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ARIMA model\n",
        "# Goal for the forecase was RSS: 1.275 and \n",
        "\n",
        "model = ARIMA(ts_log, order=(2, 1, 2))  \n",
        "results_AR = model.fit(disp=-1)  \n",
        "plt.plot(ts_diff)\n",
        "plt.plot(results_AR.fittedvalues, color='red')\n",
        "plt.title('RSS: %.4f'% sum((results_AR.fittedvalues-ts_diff)**2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yPL_V7KSybbO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds = pd.Series(results_AR.fittedvalues, copy = True)\n",
        "preds_cumsum = preds.cumsum()\n",
        "print preds_cumsum.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0n3gHBsEyj9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds_log = pd.Series(ts_log.ix[0], index=ts_log.index)\n",
        "preds_log = preds_log.add(preds_cumsum,fill_value=0)\n",
        "preds_log.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0xpr9FVHyoim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preds_ARIMA = np.exp(preds_log)\n",
        "plt.plot(ts)\n",
        "plt.plot(preds_ARIMA)\n",
        "plt.title('RMSE: %.4f'% np.sqrt(sum((preds_ARIMA-ts)**2)/len(ts)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgAwUk5iy00D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The results has a .443 confidences score, not very good but it makes sense with the amount of data. With more work and time with our datasets we will be able to train the agent and add our revised ARIMA model to act as a major argument to our requirements function(Functions that restrict the model form simply placing things everywhere, for example a cost model that allows the agent to spend a max budget per a station region)"
      ]
    },
    {
      "metadata": {
        "id": "RTXPSnoV1N30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following is our agent so far but due to time constraints at HackMIT we didnt work on the training, evaluation and functions for the agent"
      ]
    },
    {
      "metadata": {
        "id": "spUo0L_o1E-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "class Agent:\n",
        "\tdef __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
        "\t\tself.state_size = state_size # normalized previous days\n",
        "\t\tself.action_size = 3 # sit, buy, sell\n",
        "\t\tself.memory = deque(maxlen=1000)\n",
        "\t\tself.inventory = []\n",
        "\t\tself.model_name = model_name\n",
        "\t\tself.is_eval = is_eval\n",
        "\n",
        "\t\tself.gamma = 0.95\n",
        "\t\tself.epsilon = 1.0\n",
        "\t\tself.epsilon_min = 0.01\n",
        "\t\tself.epsilon_decay = 0.995\n",
        "\n",
        "\t\tself.model = load_model(\"models/\" + model_name) if is_eval else self._model()\n",
        "\n",
        "\tdef _model(self):\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Dense(units=64, input_dim=self.state_size, activation=\"relu\"))\n",
        "\t\tmodel.add(Dense(units=32, activation=\"relu\"))\n",
        "\t\tmodel.add(Dense(units=8, activation=\"relu\"))\n",
        "\t\tmodel.add(Dense(self.action_size, activation=\"linear\"))\n",
        "\t\tmodel.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
        "\n",
        "\t\treturn model\n",
        "\n",
        "\tdef act(self, state):\n",
        "\t\tif not self.is_eval and random.random() <= self.epsilon:\n",
        "\t\t\treturn random.randrange(self.action_size)\n",
        "\n",
        "\t\toptions = self.model.predict(state)\n",
        "\t\treturn np.argmax(options[0])\n",
        "\n",
        "\tdef expReplay(self, batch_size):\n",
        "\t\tmini_batch = []\n",
        "\t\tl = len(self.memory)\n",
        "\t\tfor i in range(l - batch_size + 1, l):\n",
        "\t\t\tmini_batch.append(self.memory[i])\n",
        "\n",
        "\t\tfor state, action, reward, next_state, done in mini_batch:\n",
        "\t\t\ttarget = reward\n",
        "\t\t\tif not done:\n",
        "\t\t\t\ttarget = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
        "\n",
        "\t\t\ttarget_f = self.model.predict(state)\n",
        "\t\t\ttarget_f[0][action] = target\n",
        "\t\t\tself.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "\n",
        "\t\tif self.epsilon > self.epsilon_min:\n",
        "\t\t\tself.epsilon *= self.epsilon_decay \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q-p5KXXy4kpn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thanks to Ravindra for ARIMA tutorial for time-series\n",
        "\n",
        "https://github.com/aliasvishnu\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bLVZXpXz1iVu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Acknowledgements to Google Colab Import code snippets, Tensorflow Documentation, Siraj Raval ARIMA Stock Market forcasting video, Malaria Atlas Project (MAP), IBM Research Kenya, WHO, NASA GPM, Icesat, Lansat, Consults by Ms. Wright, Ms. Battle, Ms. Casasanto\n"
      ]
    },
    {
      "metadata": {
        "id": "tflXgvCi2fCR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And than you to HackMIT 2018 for an amazing and Inspiring weekend, and the IBM, Microsoft mentors"
      ]
    }
  ]
}